# Default values for inference-charts
# This is a YAML-formatted file.

global:
  # Common settings across all inference types
  image:
    pullPolicy: IfNotPresent

  # Common resource settings
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

fluentbit:
  image:
    repository: fluent/fluent-bit
    tag: 3.2.2
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 128Mi

vllm:
  loadFormat: # runai_streamer
  logLevel: debug
  port: 8004

# The name of the model from Hugging Face
model: NousResearch/Llama-3.2-1B

# Model parameters to the inference engine
modelParameters:

# Inference configuration
inference:
  serviceName: inference
  serviceNamespace: default

  # Accelerator type: gpu or neuron
  accelerator: gpu

  # Framework type: vllm, ray-vllm, triton-vllm, aibrix, or lws-vllm
  framework:

  #Ray Specific Options
  rayOptions:
    rayVersion: 2.47.0
    # Ray native autoscaling configuration
    autoscaling:
      enabled: false
      # Ray autoscaler specific settings
      upscalingMode: "Default"
      idleTimeoutSeconds: 60  # How long to wait before scaling down idle nodes
      actorAutoscaling:
        minActors: 1
        maxActors: 1
    gcs:
      highAvailability:
        enabled: false
        redis:
          address: redis.redis
          port: 6379
          secretName:
          secretPasswordKey:
    observability:
      rayPrometheusHost: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
      rayGrafanaHost: http://kube-prometheus-stack-grafana.monitoring.svc.cluster.local
      rayGrafanaIframeHost: http://localhost:3000

  modelServer:
    image:
      repository: vllm/vllm-openai
      tag: latest
    deployment:
      replicas: 1
      maxReplicas: 2
      minReplicas: 1
      resources:
        gpu:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
        neuron:
          requests:
            aws.amazon.com/neuron: 1
          limits:
            aws.amazon.com/neuron: 1
      # NIM-specific configuration (NVIDIA Inference Microservices)
      # These are default values that can be overridden in model-specific values files
      nim:
        # Common NIM environment variables
        env:
          NVIDIA_VISIBLE_DEVICES: "all"
          NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
          NIM_CACHE_PATH: "/opt/nim/.cache"

        # Run as non-root user (required for NIM containers - do not comment out)
        # NIM images are built to run as UID 1000 with pre-configured library paths
        # Running as root will cause CUDA library loading failures
        podSecurityContext:
          fsGroup: 1000
          runAsUser: 1000
          runAsGroup: 1000

        # Shared memory configuration
        shm:
          enabled: true
          medium: Memory
          sizeLimit: 1Gi

        # Health probes - common endpoints for NIM
        livenessProbe:
          httpGet:
            path: /v1/health/live
            port: 8000
          initialDelaySeconds: 900  # Default for smaller models
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /v1/health/ready
            port: 8000
          initialDelaySeconds: 900  # Default for smaller models
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3

        # NGC and HuggingFace secrets
        secrets:
          ngcApiKey:
            name: ngc-api
            key: NGC_API_KEY
          hfToken:
            name: hf-token
            key: HF_TOKEN
      # Topology constraints for pod scheduling
      topologySpreadConstraints:
        enabled: true
        # Default constraints for Ray deployments:
        # 1. Prefer workers in same AZ as head (soft constraint)
        # 2. Require workers to be grouped together (hard constraint)
        constraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
            labelSelector:
              matchLabels: {} # Will be populated with deployment labels
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels: {} # Will be populated with worker-specific labels
      # Pod affinity for Karpenter - helps with node provisioning decisions
      podAffinity:
        enabled: true
        # Strong preference for same AZ (helps Karpenter understand intent)
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: topology.kubernetes.io/zone
              labelSelector:
                matchLabels: {} # Will be populated with deployment labels
      startupProbe:
        failureThreshold: 60
        periodSeconds: 10
        httpGet:
          path: /health
          port: 8000
    env: {}

s3ModelCopy:
  model:
  s3Path:
  namespace:
  instanceType: # run the job on the specified instance
  serviceAccountName: # serviceAccountName to be used for model uploading to S3

# serviceAccountName to use for deployments
serviceAccountName: default

# Service configuration
service:
  type: ClusterIP
  port: 8000
  annotations: {}

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
  tls: []
