# Llama 4 Scout (17B-16E) on Trainium2
# Requires pre-compiled model artifacts - see NxD Inference tutorial
# https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/llama4-tutorial.html

model: meta-llama/Llama-4-Scout-17B-16E-Instruct

modelParameters:
  maxModelLen: 16384
  maxNumSeqs: 1
  tensorParallelSize: 64

inference:
  serviceName: llama-4-scout-17b-vllm-nrn
  serviceNamespace: default
  accelerator: neuron
  framework: vllm

  modelServer:
    image:
      repository: public.ecr.aws/neuron/pytorch-inference-neuronx
      tag: 2.5.1-neuronx-py310-sdk2.21.0-ubuntu22.04
    deployment:
      resources:
        neuron:
          requests:
            aws.amazon.com/neuron: 32
            memory: 512Gi
          limits:
            aws.amazon.com/neuron: 32
            memory: 768Gi
      nodeSelector:
        node.kubernetes.io/instance-type: trn2.48xlarge
      tolerations:
        - key: aws.amazon.com/neuron
          operator: Exists
          effect: NoSchedule
    env:
      VLLM_USE_V1: "1"
      NEURON_COMPILED_ARTIFACTS: ""  # Set via --set flag
