# NVIDIA NIM Llama 3.1 8B Instruct
# Optimized for GPU inference with TensorRT-LLM

# NIM doesn't use the model parameter the same way - it's baked into the container
model: meta/llama3-8b-instruct

inference:
  serviceName: nim-llama-31-8b
  serviceNamespace: default # Recommended: Move to a dedicated namespace (e.g., 'nim') to ensure service isolation.
  accelerator: gpu
  framework: nim

  modelServer:
    image:
      repository: nvcr.io/nim/meta/llama3-8b-instruct
      tag: latest
    
    deployment:
      replicas: 1
      instanceType: g6e.2xlarge  # 1 GPU instance suitable for 8B model
      
      resources:
        gpu:
          requests:
            cpu: "4"
            memory: "32Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "1"
      
      # NIM-specific configuration
      nim:
        # Persistent volume for model cache 
        # critical for performance - prevents repeated model loading
        persistence:
          enabled: true
          size: 200Gi  # Llama 3 8B requires less storage than SD models
          storageClassName: ""  # Use default storage class
          claimName: nim-cache-pvc
        
        # Init containers for cache cleanup & management
        # recommended for issues with model caching
        initContainers:
          fixPermissions:
            enabled: true
            image: busybox:1.35
          clearTrtCache:
            enabled: true
            image: busybox:1.35
        
        # NIM environment variables for LLM
        env:
          # For full list: https://docs.nvidia.com/nim/large-language-models/latest/configuration.html
          NIM_MODEL_NAME: "meta/llama3-8b-instruct"
          NVIDIA_VISIBLE_DEVICES: "all"
          NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
          NIM_CACHE_PATH: "/opt/nim/.cache"
          # LLM-specific settings
          NIM_MAX_MODEL_LEN: "4096"  # Maximum sequence length
          NIM_TENSOR_PARALLEL_SIZE: "1"  # Number of GPUs for tensor parallelism
        
        # Security context
        securityContext:
          capabilities:
            add: ["SYS_ADMIN"]
          runAsUser: 1000
          runAsGroup: 1000
        
        podSecurityContext:
          fsGroup: 1000
          runAsUser: 1000
          runAsGroup: 1000
        
        # Shared memory configuration
        shm:
          enabled: true
          medium: Memory
          sizeLimit: 1Gi
        
        # Health probes (NIM LLM takes 10-15 minutes to start)
        livenessProbe:
          httpGet:
            path: /v1/health/live
            port: 8000
          initialDelaySeconds: 900  # 15 minutes for LLM
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /v1/health/ready
            port: 8000
          # LLMs typically load faster than diffusion models
          initialDelaySeconds: 900  # 15 minutes
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
        
        # NGC and HF secrets
        secrets:
          ngcApiKey:
            name: ngc-api
            key: NGC_API_KEY
          hfToken:
            name: hf-token
            key: HF_TOKEN
      
      # Optimized for Llama 3: Works on Ampere (A), Lovelace (L), Hopper (H), and Blackwell (B) architectures.
      # g6e instances utilize L40S (Lovelace) GPUs.
      # For 8B model, g6e.2xlarge (1 GPU) is sufficient
      nodeSelector:
        karpenter.sh/nodepool: g6e-nvidia
      
      # Tolerations
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "karpenter.sh/nodepool"
          operator: "Equal"
          value: "g6e-nvidia"
          effect: "NoSchedule"
      
      # Pod annotations
      annotations:
        karpenter.sh/do-not-evict: "true"
      
      # Affinity - prefer on-demand instances, adjust as needed
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: karpenter.sh/capacity-type
                    operator: In
                    values:
                      - on-demand

# Image pull secrets for NGC registry
imagePullSecrets:
  - name: ngc-secret
